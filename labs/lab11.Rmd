---
title: 'ETC5512: Instructions for Lab 11'
author: "Di Cook"
date: "Week 11"
output: 
  html_document:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE, 
  warning = FALSE,
  error = FALSE, 
  out.width = "70%",
  fig.width = 8, 
  fig.height = 6,
  fig.retina = 3)
set.seed(6)
filter <- dplyr::filter
```

## `r emo::ji("target")` Objectives

In this tutorial, you will learn 

- how to scrape data from a web site.
- learn some html, to understand how the scraping can be achieved.
- use web browser developer tools to inspect elements of a web page

## `r emo::ji("wrench")` Preparation  

1. If you haven't already got Google Chrome on your computer, install it. This web browser has the best developer tools, so that you can inspect the details of a web site, to structure code to collect the data. 
2. Install these R packages: `rvest`,  and `cricketdata`. 
3. If you have not seen `html` before, its worth it to work through [this beginners guide](https://htmldog.com/guides/html/beginner/). You don't need to be an html expert in order to scrape web pages, but its good to have some basics. If can be also useful to learn about `css` which is used to define the style for a web page and for this see [beginner CSS tutorial on the same site](https://htmldog.com/guides/css/beginner/). 
4. Read the `rvest` documentation at https://rvest.tidyverse.org. There are two documents, "Harvesting the web with rvest" and "SelectorGadget". Both are important.  
5. For some motivation on webscraping, browse this recent blog post by [Ryo](https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data-with-R/) for politely scraping soccer data with `rvest`.

## `r emo::ji("earth")` Web sites we are visiting

- [Asian Cup soccer](https://en.wikipedia.org/wiki/AFC_Asian_Cup_records_and_statistics)
- [CricInfo](http://stats.espncricinfo.com)
- [Women's tennis](https://www.wtatennis.com/stats)

Check the `Rmd` file for code to do polite introductions to these web sites. 

```{r eval=FALSE}
# This code can be run to check the web site scraping rules
library(polite)
soccer_bow <- bow(
  url = "https://en.wikipedia.org/wiki/AFC_Asian_Cup_records_and_statistics",  # base URL
  user_agent = "Wild-caught Data <https://wcd.numbat.space>",  # identify ourselves
  force = TRUE
)
soccer_bow
cricket_bow <- bow(
  url = "http://stats.espncricinfo.com",  # base URL
  user_agent = "Wild-caught Data <https://wcd.numbat.space>",  # identify ourselves
  force = TRUE
)
cricket_bow
tennis_bow <- bow(
  url = "https://www.wtatennis.com/stats",  # base URL
  user_agent = "Wild-caught Data <https://wcd.numbat.space>",  # identify ourselves
  force = TRUE
)
tennis_bow
```
## `r emo::ji("soccer")` Exercise 11A

Motivated by Ryo's blog post, we are going to scrape soccer records from Wikipedia. 

a. We're going to first extract records on Asian Cup men's soccer. This can be done with the code below.

```{r}
library(tidyverse)
library(rvest)
library(plotly)

url <- "https://en.wikipedia.org/wiki/AFC_Asian_Cup_records_and_statistics"
html <- read_html(url)
tbl <-  html_nodes(html, "table") 
alltime_stats <- html_table(tbl[[6]], fill=TRUE)
```

b. How can we know that the data is organised as a table in the html? Go to the web site in your Google Chrome browser and use InspectorGadget to see the elements in the html.

c. How did we know to extract the data from the 6th table? Try changing the `6` in the code `html_table(tbl[[6]], fill=TRUE)` and compare the result with the tables presented in the web page.

d. Let's use the data to make a chart. Examine the goals for and against, of each team. add a guide line where the for and against are equal. Also make the plot interactive, using plotly,  so you can browse over team names. Which teams have scored more goals than have been scored against them, over all time? How does this compare with the champions list?

```{r fig.width=6, fig.height=4, eval=FALSE}
p <- ggplot(alltime_stats, aes(x=???, y=???, label=???)) + 
  geom_abline(intercept=0, slope=1) +
  geom_point() + theme(aspect.ratio=1)
ggplotly(???)
```

e. How did we know that these were tables in the web page? Here's where its important to learn how to use the developer tools in your web browser. Open the page in Google Chrome, scroll down to the table of interest, and right click to choose `Inspect`:

<img src="images/scraping_soccer.png">

This will bring up the page source. Can you see the `<table` tag? and see the same information that is visible in the web page itself?

<img src="images/scraping_soccer2.png">


## `r emo::ji("cricket")` Exercise 11B

a. Using the `fetch_cricinfo` function from the `cricketdata` package, extract all the records for Australian women's T20 matches. And answer the following questions:

```{r echo=TRUE, eval=FALSE}
# remotes::install_github("ropenscilabs/cricketdata")
library(cricketdata)
auswt20 <- fetch_cricinfo("T20", "Women", country="Aust")
```
- How many players statistics are recorded?
- What was the first year of records and the last year?
- What player has played the most matches?
- Do players with high strike rate generally have high batting averages too?

```{r eval=FALSE}
auswt20 %>% summarise(???, ???)
auswt20 %>% arrange(???) %>% select(???, Matches)
auswt20 %>% ggplot(aes(x=???, y=???)) + 
  geom_point() +
  ylim(0, 60)
```

b. Take a look at the code for the function `fetch_cricinfo`. The work is mostly done by another hidden function `cricketdata:::fetch_cricket_data`. A key part of this function occurs in these lines:

```
url <- paste0("http://stats.espncricinfo.com/ci/engine/stats/index.html?class=", 
            matchclass, ifelse(is.null(country), "", 
            paste0(";team=", 
                team)), ";page=", 
                format(page, scientific = FALSE), 
            ";template=results;type=", activity, 
            view_text, ";size=200;wrappertype=print")
```

Try setting these values, and creating the URL manually. When you have it right, you will have found the page that the data is extracted from! (Alternatively, if this is too frustrating try working with the "statguru" query tool to find the table of interest.)

```{r eval=FALSE}
matchtype <- ???
country <- "aust"
sex <- ???
activity <- "batting"
view <- "career"

# this is internally calculated in the package!
matchclass <- match(matchtype, c("test", "odi", "t20")) + 
        7 * (sex == "women")
team <- 289 
```

Using your URL, use the `read_html` and `html_table` functions to reproduce what the `cricketdata` package did for you.

```{r eval=FALSE}
url <- "https://stats.espncricinfo.com/ci/engine/stats/index.html?class=10;team=289;template=results;type=batting"
raw <- try(xml2::read_html(url), silent = TRUE)
tables <- rvest::html_table(raw, fill = TRUE)
tables[[3]]
```

`r emo::ji("thinking")` Use the inspect tool on the  web page, to see that it is indeed an html table. 

## `r emo::ji("tennis")` Exercise 11C

Sometimes pages are dynamically created, which means that it isn't possible too directly extract the data. The women's tennis records web site uses dynamic web pages. 

a. Have a look at what happens when you directly try to read the women's stats web page. 

```{r  echo=TRUE, eval=FALSE}
url <- "https://www.wtatennis.com/stats"
wta_html <- read_html(url)
wta_rankings <- html_node(wta_html, "table")
```

b. Now save a copy of the page, and re-try to read it.

```{r echo=TRUE}
# Save web page source locally, because it contains javascript content
wta_html <- read_html("wta_rankings2.htm")
wta_rankings <- html_node(wta_html, "table") %>% html_table(fill=TRUE) 
# There is only one table in page so use html_node rather than html_nodes
wta_rankings <- wta_rankings %>% 
  janitor::remove_empty() %>% 
  as_tibble()
```

This time you've got data. How can you tell that the page is dynamic? Inspect the source, and you will find `script`. This often indicates that some javascript is used to create the table. 

c. Use your wrangling skills to clean up the data, making numeric variables numeric

```{r eval=FALSE}
wta_rankings <- wta_rankings %>%
  mutate(???) %>%
  mutate(???)
```


d. Make a plot with the data you've just extracted. Your choice. 



